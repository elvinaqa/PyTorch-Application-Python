# PyTorch-Application-Python
PyTorch Application Python

"""Backpropogration is the process of going back after the first run in the neural networks 
to update the weights and biases"""

# We want to change the weights so that loss function is minimized - Gradient Descent

"""L-3 L-2 L-1 L

Between each L, those happen below:
    1) X is input which is given to function Z
    2) z = wx + b Function z convert X by adding up W weight and B bias 
    3) activation function applied: sigma(z) - output of Z is given to activation function
    4) output is activation function is given to next neuron - layer
    """

![Backpropogation Step4](https://user-images.githubusercontent.com/57037068/85925922-a75d0b80-b8ac-11ea-84e4-ea257f55a51a.PNG)
